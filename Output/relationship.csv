knowledge_id,knowledge_name,concept_id,concept_name,relation
1,Transformer模型,1,自注意力机制,完全替代循环神经网络（RNN）和卷积神经网络（CNN）
2,自注意力机制,2,缩放点积注意力,通过查询（Q）、键（K）、值（V）矩阵计算权重并缩放√dk缓解梯度消失
3,多头注意力,3,并行子空间投影,将输入线性投影到多个子空间并行计算注意力后拼接结果
4,位置编码,4,正弦/余弦函数,将绝对位置信息编码为向量并与输入嵌入相加以保留序列顺序
5,编码器-解码器架构,5,堆叠层结构,编码器含自注意力层和前馈网络，解码器增加掩码注意力防止信息泄露
6,训练效率,6,并行化计算,因无需序列递归可实现更高并行度，显著减少训练时间（如8 GPU训练3.5天达到SOTA）
7,实验结果,7,BLEU指标,在WMT2014英德/英法翻译任务分别达到28.4和41.0 BLEU，超越传统模型
